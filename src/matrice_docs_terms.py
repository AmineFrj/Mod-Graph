# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nABCOJ5bUabeR8kv0stBMNX5-tc9ThA-
"""


import pandas as pd

def generate_doc_terms(auteurs_csv_path = "data/Liste_Auteurs_affiliation_photo.csv",
                       articles_csv_path = "data/export_articles_EGC_2004_2018 (2).csv",
                       verbose = False):

    data_auteurs=pd.read_csv(auteurs_csv_path,sep=';')
    data_articles=pd.read_csv(articles_csv_path,sep='\t')
    if verbose:
        data_articles.head()
        data_articles.columns
        len(data_articles)

    data_articles=data_articles[data_articles['abstract'].notna()]
    if verbose:
        len(data_auteursrticles)

    data_articles['title']=data_articles['title'].apply(lambda x: ' '.join(x.lower() for x in x.split() ) )
    if verbose:
        data_articles['title']

    import string
    data_articles['title']=data_articles['title'].apply(lambda x: ' '.join(x for x in x.split() if not x.isdigit() ))
    data_articles['title']=data_articles['title'].str.replace('[^\w\s]'," ")
    if verbose:
        data_articles['title']

    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import sent_tokenize, word_tokenize
    nltk.download('stopwords')

    english_stopwords=set(stopwords.words('english'))
    french_stopwords = set(stopwords.words('french'))

    data_articles['title']=data_articles['title'].apply(lambda x: ' '.join(x for x in x.split() if x not in french_stopwords ))
    data_articles['title']=data_articles['title'].apply(lambda x: ' '.join(x for x in x.split() if x not in english_stopwords ))

    from sklearn.feature_extraction.text import CountVectorizer
    from nltk.tokenize import RegexpTokenizer
    #tokenizer to remove unwanted elements from out data like symbols and numbers

    cv = CountVectorizer()
    text_counts= cv.fit_transform(data_articles['title'])

    M=text_counts.toarray()

    M[M>1]=1

    import numpy as np

    res_path_doc_term = "data/results/Doc_Term_Matrix.csv"
    res_path_doc_aut = "/data/results/docAut.csv"
    
    print(f"saving results to {res_path_doc_term} and {res_path_doc_aut}")
    np.savetxt(res_path_doc_term, M, delimiter=";")

    print("AM")
    A=pd.read_csv("data/results/docAut.csv",sep="\t")

    print("AT")
    AT=A.T

    print("E")
    E=AT.dot(M)

    if not verbose:
        print("Somme des lignes nulles",np.sum(np.sum(E)==0), "sur ", len(E), "lignes en tout")

    print("Calcultating co term matrix")
    co_term = E.dot(E.T)

    path_co_term = "data/results/co_term.csv"
    print(f"saving results to {path_co_term}")
    co_term.to_csv(path_co_term, sep = ",")

if __name__ == "__main__":

    generate_doc_terms()
